# CIFAR-10 Project GitLab CI/CD Pipeline

stages:
  - quality
  - validation
  - test
  - benchmark
  - security
  - deploy

variables:
  PYTHON_VERSION: "3.10"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  CUDA_VERSION: "11.8"

cache:
  paths:
    - .cache/pip
    - .cache/torch
    - .cache/uv
    - .venv/

# Code Quality and Testing
code-quality:
  stage: quality
  image: python:${PYTHON_VERSION}
  
  before_script:
    # Install uv
    - curl -LsSf https://astral.sh/uv/install.sh | sh
    - export PATH="$HOME/.cargo/bin:$PATH"
    # Install dependencies with uv
    - uv sync --extra dev
    
  script:
    - echo "Running code quality checks..."
    
    # Code formatting
    - echo "🔍 Checking code formatting with Black..."
    - uv run black --check --diff src/ --line-length 100
    
    # Linting and formatting with ruff (replaces flake8 and isort)
    - echo "🧹 Linting and checking imports with ruff..."
    - uv run ruff check src/
    - uv run ruff format --check src/
    
    # Type checking
    - echo "🔬 Type checking with mypy..."
    - uv run mypy src/ --ignore-missing-imports --no-strict-optional
    
    # Security check
    - echo "🔒 Security check with bandit..."
    - uv run bandit -r src/ -ll
    
    # Dependency vulnerability check
    - echo "🛡️ Dependency vulnerability check..."
    - uv run safety check || echo "Safety check completed with warnings"
    
    # Unit tests
    - echo "🧪 Running unit tests..."
    - mkdir -p tests
    - echo "import pytest" > tests/__init__.py
    - echo "def test_imports():" > tests/test_basic.py
    - echo "    from src import enums, model, data_loader" >> tests/test_basic.py
    - echo "    assert True" >> tests/test_basic.py
    - uv run pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
    
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - htmlcov/
    expire_in: 1 week
    
  coverage: '/TOTAL.*\s+(\d+%)$/'

# Data Validation
data-validation:
  stage: validation
  image: python:${PYTHON_VERSION}
  needs: ["code-quality"]
  
  before_script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    
  script:
    - echo "🔍 Validating CIFAR-10 dataset..."
    
    # Download and validate dataset
    - python -c "
      import torch;
      import torchvision;
      from src.data_loader import load_raw_cifar10;
      print('📥 Downloading CIFAR-10...');
      train_set, test_set = load_raw_cifar10();
      print(f'✅ Train set: {len(train_set)} samples');
      print(f'✅ Test set: {len(test_set)} samples');"
    
    # Validate dataset integrity
    - python -c "
      from src.data_loader import load_raw_cifar10;
      from src.validation import DatasetValidator;
      print('🔍 Validating dataset integrity...');
      train_set, _ = load_raw_cifar10();
      validator = DatasetValidator();
      report = validator.validate_cifar10_dataset(train_set);
      print(f'Dataset validation: {\"✅ PASSED\" if report.overall_passed else \"❌ FAILED\"}');
      print(f'Total samples: {report.total_samples}');
      print(f'Shape validation: {\"✅\" if report.shape_validation.passed else \"❌\"} {report.shape_validation.message}');
      print(f'Class distribution: {\"✅\" if report.class_distribution.passed else \"❌\"} {report.class_distribution.message}');
      if not report.overall_passed: exit(1);"
    
    # Test data loaders for each model type
    - python -c "
      from src.data_loader import load_data, load_raw_cifar10;
      from src.enums import ModelType;
      from src.validation import validate_dataset_preprocessing;
      print('🔍 Testing data loaders...');
      for model_type in [ModelType.RESNET, ModelType.EFFICIENTNET, ModelType.VIT]:
          print(f'Testing {model_type.name} data loader...');
          train_loader, val_loader, test_loader = load_data(
              model_type=model_type, batch_size=32, val_ratio=0.1, num_workers=0
          );
          train_batch = next(iter(train_loader));
          val_batch = next(iter(val_loader));
          print(f'  ✅ Train batch: {train_batch[0].shape}');
          print(f'  ✅ Val batch: {val_batch[0].shape}');
          raw_dataset, _ = load_raw_cifar10();
          validation_passed = validate_dataset_preprocessing(raw_dataset, train_loader);
          if not validation_passed:
              print(f'  ❌ Validation failed for {model_type.name}'); exit(1);
          else: print(f'  ✅ Validation passed for {model_type.name}');
      print('🎉 All data loader tests passed!');"
      
  artifacts:
    paths:
      - data/
    expire_in: 1 day

# Model Architecture Validation
model-validation:
  stage: validation
  image: python:${PYTHON_VERSION}
  needs: ["code-quality"]
  
  before_script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    
  script:
    - echo "🏗️ Validating model architectures..."
    
    # Test model creation and forward passes
    - python -c "
      import torch;
      from src.model import get_model;
      from src.enums import ModelType;
      from src.layer_freezing import PRESET_CONFIGS;
      print('🔍 Testing model creation...');
      device = torch.device('cpu');
      for model_type in [ModelType.RESNET, ModelType.EFFICIENTNET, ModelType.VIT]:
          print(f'Testing {model_type.name}...');
          model = get_model(model_type, num_classes=10, pretrained=True);
          model.to(device);
          dummy_input = torch.randn(2, 3, 224, 224).to(device);
          output = model(dummy_input);
          assert output.shape == (2, 10), f'Output shape mismatch: {output.shape}';
          print(f'  ✅ Basic model test passed - Output: {output.shape}');
          for config_name, freeze_config in PRESET_CONFIGS.items():
              if config_name in ['no_freezing', 'full_freeze']:
                  model_frozen = get_model(model_type, num_classes=10, pretrained=True, freezing_config=freeze_config);
                  model_frozen.to(device);
                  output_frozen = model_frozen(dummy_input);
                  assert output_frozen.shape == (2, 10);
                  print(f'  ✅ Freezing config \"{config_name}\" passed');
      print('🎉 All model tests passed!');"
    
    # Test layer freezing functionality
    - python -c "
      import torch;
      from src.model import get_model;
      from src.enums import ModelType;
      from src.layer_freezing import LayerFreezingManager, FreezingConfig, FreezingStrategy;
      print('🔍 Testing layer freezing...');
      model = get_model(ModelType.RESNET, num_classes=10, pretrained=True);
      config = FreezingConfig(strategy=FreezingStrategy.FULL_FREEZE);
      manager = LayerFreezingManager(model, config);
      summary = manager.get_freezing_summary();
      print(f'  📊 Total parameters: {summary[\"total_parameters\"]:,}');
      print(f'  🔓 Trainable: {summary[\"trainable_parameters\"]:,} ({summary[\"trainable_percentage\"]:.1f}%)');
      print(f'  🔒 Frozen: {summary[\"frozen_parameters\"]:,}');
      assert summary['trainable_percentage'] < 50, 'Full freeze should have <50% trainable params';
      print('  ✅ Layer freezing test passed');"

# Training Smoke Test
training-smoke-test:
  stage: test
  image: python:${PYTHON_VERSION}
  needs: ["data-validation", "model-validation"]
  
  before_script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    
  script:
    - echo "🚀 Running training smoke test..."
    
    # Quick training test with minimal data
    - python -c "
      import torch;
      import torch.nn as nn;
      from torch.utils.data import DataLoader, TensorDataset;
      from src.data_loader import load_data;
      from src.model import get_model;
      from src.enums import ModelType;
      from src.evaluate import evaluate;
      print('🔥 Running training smoke test...');
      device = torch.device('cpu');
      train_loader, val_loader, _ = load_data(ModelType.RESNET, batch_size=8, val_ratio=0.2, num_workers=0);
      train_samples = [];
      val_samples = [];
      for i, batch in enumerate(train_loader):
          if i >= 2: break;
          train_samples.append(batch);
      for i, batch in enumerate(val_loader):
          if i >= 1: break;
          val_samples.append(batch);
      train_images = torch.cat([batch[0] for batch in train_samples]);
      train_labels = torch.cat([batch[1] for batch in train_samples]);
      val_images = torch.cat([batch[0] for batch in val_samples]);
      val_labels = torch.cat([batch[1] for batch in val_samples]);
      small_train_loader = DataLoader(TensorDataset(train_images, train_labels), batch_size=8, shuffle=True);
      small_val_loader = DataLoader(TensorDataset(val_images, val_labels), batch_size=8, shuffle=False);
      model = get_model(ModelType.RESNET, num_classes=10, pretrained=False);
      model.to(device);
      optimizer = torch.optim.SGD(model.parameters(), lr=0.01);
      criterion = nn.CrossEntropyLoss();
      print(f'🔄 Training on {len(train_images)} samples, validating on {len(val_images)} samples');
      model.train();
      total_loss = 0;
      num_batches = 0;
      for images, labels in small_train_loader:
          images, labels = images.to(device), labels.to(device);
          optimizer.zero_grad();
          outputs = model(images);
          loss = criterion(outputs, labels);
          loss.backward();
          optimizer.step();
          total_loss += loss.item();
          num_batches += 1;
      avg_loss = total_loss / num_batches if num_batches > 0 else 0;
      print(f'📊 Average training loss: {avg_loss:.4f}');
      val_loss, accuracy, f1 = evaluate(model, small_val_loader, criterion, device);
      print(f'📊 Validation - Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}');
      assert avg_loss > 0, 'Training loss should be positive';
      assert 0 <= accuracy <= 1, f'Accuracy should be [0,1], got {accuracy}';
      assert 0 <= f1 <= 1, f'F1 should be [0,1], got {f1}';
      print('✅ Training smoke test passed!');"

# EDA Functionality Test
eda-test:
  stage: test
  image: python:${PYTHON_VERSION}
  needs: ["data-validation"]
  
  before_script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    
  script:
    - echo "📊 Testing EDA functionality..."
    
    - python -c "
      from src.data_loader import load_raw_cifar10, load_data;
      from src.eda import DatasetAnalyzer, validate_dataset_preprocessing;
      from src.enums import ModelType;
      print('📊 Testing EDA functionality...');
      train_set, _ = load_raw_cifar10();
      analyzer = DatasetAnalyzer(save_dir='test_eda_output');
      summary = analyzer.quick_dataset_summary(train_set);
      print(f'📈 Dataset: {summary[\"total_samples\"]} samples, {summary[\"num_classes\"]} classes');
      print(f'⚖️ Balanced: {summary[\"is_balanced\"]}');
      train_loader, _, _ = load_data(ModelType.RESNET, batch_size=16, num_workers=0);
      loader_analysis = analyzer.analyze_data_loader_batch(train_loader, num_batches=2);
      print(f'📊 Analyzed: {loader_analysis[\"num_batches_analyzed\"]} batches');
      print(f'📏 Value range: [{loader_analysis[\"value_range\"][\"min\"]:.3f}, {loader_analysis[\"value_range\"][\"max\"]:.3f}]');
      assert not loader_analysis[\"data_quality\"][\"has_nan\"], 'Data should not contain NaN';
      assert not loader_analysis[\"data_quality\"][\"has_inf\"], 'Data should not contain Inf';
      print('✅ EDA functionality test passed!');"
      
  artifacts:
    paths:
      - test_eda_output/
    expire_in: 3 days

# Performance Benchmark
performance-benchmark:
  stage: benchmark
  image: python:${PYTHON_VERSION}
  needs: ["training-smoke-test"]
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_PIPELINE_SOURCE == "web"
    
  before_script:
    - python -m pip install --upgrade pip
    - pip install -r requirements.txt
    
  script:
    - echo "⚡ Running performance benchmark..."
    
    - python -c "
      import torch;
      import time;
      import json;
      from src.model import get_model;
      from src.enums import ModelType;
      print('⚡ Benchmarking model inference...');
      device = torch.device('cpu');
      models_to_test = [
          (ModelType.RESNET, 'ResNet-50'),
          (ModelType.EFFICIENTNET, 'EfficientNet-B0'),
          (ModelType.VIT, 'Vision Transformer')
      ];
      results = [];
      for model_type, model_name in models_to_test:
          print(f'🔍 Benchmarking {model_name}...');
          model = get_model(model_type, num_classes=10, pretrained=True);
          model.to(device);
          model.eval();
          dummy_input = torch.randn(1, 3, 224, 224).to(device);
          for _ in range(5):
              with torch.no_grad(): _ = model(dummy_input);
          batch_sizes = [1, 4, 16];
          for batch_size in batch_sizes:
              input_tensor = torch.randn(batch_size, 3, 224, 224).to(device);
              times = [];
              for _ in range(10):
                  start_time = time.time();
                  with torch.no_grad(): output = model(input_tensor);
                  end_time = time.time();
                  times.append(end_time - start_time);
              avg_time = sum(times) / len(times);
              throughput = batch_size / avg_time;
              results.append({
                  'model': model_name,
                  'batch_size': batch_size,
                  'avg_time_ms': avg_time * 1000,
                  'throughput_imgs_per_sec': throughput
              });
              print(f'  📊 Batch {batch_size}: {avg_time*1000:.2f}ms, {throughput:.1f} imgs/sec');
      with open('benchmark_results.json', 'w') as f:
          json.dump(results, f, indent=2);
      print('✅ Benchmark completed!');"
      
  artifacts:
    reports:
      performance: benchmark_results.json
    paths:
      - benchmark_results.json
    expire_in: 1 week

# Security Scanning
security-scan:
  stage: security
  image: python:${PYTHON_VERSION}
  
  before_script:
    - python -m pip install --upgrade pip
    - pip install safety bandit semgrep
    
  script:
    - echo "🔒 Running security scans..."
    
    # Dependency vulnerability scan
    - echo "🛡️ Scanning dependencies for vulnerabilities..."
    - safety check --json --output safety_report.json || true
    
    # Static analysis security scan
    - echo "🔍 Running static security analysis..."
    - bandit -r src/ -f json -o bandit_report.json || true
    
    # Semgrep security rules
    - echo "🔬 Running Semgrep security analysis..."
    - semgrep --config=auto --json --output semgrep_report.json src/ || true
    
    - echo "✅ Security scans completed"
    
  artifacts:
    reports:
      sast: semgrep_report.json
    paths:
      - safety_report.json
      - bandit_report.json
      - semgrep_report.json
    expire_in: 1 week
  allow_failure: true

# Documentation Check
documentation-check:
  stage: validation
  image: alpine:latest
  
  before_script:
    - apk add --no-cache bash grep
    
  script:
    - echo "📚 Checking documentation..."
    
    # Check for required files
    - |
      echo "🔍 Checking for required documentation files..."
      required_files=("README.md" "CLAUDE.md" "requirements.txt")
      missing_files=()
      
      for file in "${required_files[@]}"; do
        if [[ ! -f "$file" ]]; then
          missing_files+=("$file")
        else
          echo "✅ Found: $file"
        fi
      done
      
      if [[ ${#missing_files[@]} -gt 0 ]]; then
        echo "❌ Missing required files:"
        printf '%s\n' "${missing_files[@]}"
        exit 1
      fi
    
    # Check README content
    - |
      echo "🔍 Checking README.md content..."
      required_sections=("setup" "usage" "model")
      readme_content=$(cat README.md | tr '[:upper:]' '[:lower:]')
      missing_sections=()
      
      for section in "${required_sections[@]}"; do
        if ! echo "$readme_content" | grep -qi "$section"; then
          missing_sections+=("$section")
        else
          echo "✅ Found section reference: $section"
        fi
      done
      
      if [[ ${#missing_sections[@]} -gt 0 ]]; then
        echo "⚠️ README.md might be missing sections:"
        printf '%s\n' "${missing_sections[@]}"
        echo "Consider adding these sections for better documentation"
      fi
    
    - echo "✅ Documentation check completed"

# Deployment (Manual trigger only)
deploy-docs:
  stage: deploy
  image: alpine:latest
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
  
  before_script:
    - apk add --no-cache rsync openssh-client
    
  script:
    - echo "🚀 Deploying documentation..."
    - echo "This would deploy documentation to a server or pages"
    - echo "✅ Documentation deployment placeholder completed"
    
  artifacts:
    paths:
      - htmlcov/
    expire_in: 1 week

# Pipeline Success Notification
pipeline-success:
  stage: .post
  image: alpine:latest
  
  script:
    - echo "🎉 CIFAR-10 CI/CD Pipeline Completed Successfully!"
    - echo "=========================="
    - echo "✅ Code Quality: Passed"
    - echo "✅ Data Validation: Passed" 
    - echo "✅ Model Validation: Passed"
    - echo "✅ Training Test: Passed"
    - echo "✅ Security Scan: Completed"
    - echo "✅ Documentation: Verified"
    - echo "=========================="
    - echo "🚀 Ready for development and experimentation!"
  
  rules:
    - if: $CI_PIPELINE_STATUS == "success"